{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attempted-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "processed-february",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Heschmat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Heschmat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Heschmat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hairy-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ranging-spanking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-middle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "atmospheric-administrator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>category</th>\n",
       "      <th>category:confidence</th>\n",
       "      <th>category_gold</th>\n",
       "      <th>id</th>\n",
       "      <th>screenname</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>662822308</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/18/15 4:31</td>\n",
       "      <td>Information</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.365280e+17</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>Barclays CEO stresses the importance of regula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>662822309</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/18/15 13:55</td>\n",
       "      <td>Information</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.860130e+17</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>Barclays announces result of Rights Issue http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>662822310</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/18/15 8:43</td>\n",
       "      <td>Information</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.795800e+17</td>\n",
       "      <td>Barclays</td>\n",
       "      <td>Barclays publishes its prospectus for its å£5....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  662822308    False   finalized                   3      2/18/15 4:31   \n",
       "1  662822309    False   finalized                   3     2/18/15 13:55   \n",
       "2  662822310    False   finalized                   3      2/18/15 8:43   \n",
       "\n",
       "      category  category:confidence category_gold            id screenname  \\\n",
       "0  Information                  1.0           NaN  4.365280e+17   Barclays   \n",
       "1  Information                  1.0           NaN  3.860130e+17   Barclays   \n",
       "2  Information                  1.0           NaN  3.795800e+17   Barclays   \n",
       "\n",
       "                                                text  \n",
       "0  Barclays CEO stresses the importance of regula...  \n",
       "1  Barclays announces result of Rights Issue http...  \n",
       "2  Barclays publishes its prospectus for its å£5....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "studied-delhi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Information    2129\n",
       "Action          724\n",
       "Dialogue        226\n",
       "Exclude          39\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "loving-ending",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000    2430\n",
       "0.6614      35\n",
       "0.6643      33\n",
       "0.6747      32\n",
       "0.6775      29\n",
       "          ... \n",
       "0.7202       1\n",
       "0.8860       1\n",
       "0.8570       1\n",
       "0.6568       1\n",
       "0.9041       1\n",
       "Name: category:confidence, Length: 194, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category:confidence'].value_counts(dropna= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-authority",
   "metadata": {},
   "source": [
    "Only keep the rows with `confidence` of 100%. Also, remove the `category` of `Exclude`, as there are very few of them anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-coordinate",
   "metadata": {},
   "source": [
    "## 0. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "competent-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "trying-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fuzzy-limit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barclays CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  http://t.co/Ge9Lp7hpyG'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-combining",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "positive-craft",
   "metadata": {},
   "source": [
    "### 1. Clean & Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-hypothesis",
   "metadata": {},
   "source": [
    "Replace the urls in the messages with `urlplaceholder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pregnant-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RegEx for url pattern\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "specialized-shoot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barclays CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  http://t.co/Ge9Lp7hpyG'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = X[0]\n",
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "brief-title",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://t.co/Ge9Lp7hpyG']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(url_regex, msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "affected-donor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barclays CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  urlplaceholder'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://t.co/Ge9Lp7hpyG'\n",
    "msg.replace(url, 'urlplaceholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "contained-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    # get list of all urls using regex\n",
    "    detected_urls = re.findall(url_regex, txt)\n",
    "    \n",
    "    # replace each url in text string with placeholder\n",
    "    for url in detected_urls:\n",
    "        txt = txt.replace(url, 'urlplaceholder')\n",
    "\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(txt.lower())\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer\n",
    "    # Lemmatize and remove the white spaces - leading & trailing\n",
    "    tokens_clean = [lemmatizer().lemmatize(token.strip()) for token in tokens]\n",
    "\n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "entitled-license",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barclays CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  http://t.co/Ge9Lp7hpyG\n",
      "=======>  ['barclays', 'ceo', 'stress', 'the', 'importance', 'of', 'regulatory', 'and', 'cultural', 'reform', 'in', 'financial', 'service', 'at', 'brussels', 'conference', 'urlplaceholder'] \n",
      "\n",
      "========================================================================\n",
      "Barclays announces result of Rights Issue http://t.co/LbIqqh3wwG\n",
      "=======>  ['barclays', 'announces', 'result', 'of', 'right', 'issue', 'urlplaceholder'] \n",
      "\n",
      "========================================================================\n",
      "Barclays publishes its prospectus for its å£5.8bn Rights Issue: http://t.co/YZk24iE8G6\n",
      "=======>  ['barclays', 'publishes', 'it', 'prospectus', 'for', 'it', 'å£5.8bn', 'right', 'issue', ':', 'urlplaceholder'] \n",
      "\n",
      "========================================================================\n",
      "Barclays Group Finance Director Chris Lucas is to step down at the end of the week due to ill health http://t.co/nkuHoAfnSD\n",
      "=======>  ['barclays', 'group', 'finance', 'director', 'chris', 'lucas', 'is', 'to', 'step', 'down', 'at', 'the', 'end', 'of', 'the', 'week', 'due', 'to', 'ill', 'health', 'urlplaceholder'] \n",
      "\n",
      "========================================================================\n",
      "Barclays announces that Irene McDermott Brown has been appointed as Group Human Resources Director http://t.co/c3fNGY6NMT\n",
      "=======>  ['barclays', 'announces', 'that', 'irene', 'mcdermott', 'brown', 'ha', 'been', 'appointed', 'a', 'group', 'human', 'resource', 'director', 'urlplaceholder'] \n",
      "\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "# test out function\n",
    "X, y = load_data()\n",
    "for msg in X[:5]:\n",
    "    tokens = tokenize(msg)\n",
    "    print(msg)\n",
    "    print('=======> ', tokens, '\\n')\n",
    "    print('=' * 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-aberdeen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "blank-solomon",
   "metadata": {},
   "source": [
    "## 2. Machine Learning Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-cincinnati",
   "metadata": {},
   "source": [
    "### Step 1: Load data and perform a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "crucial-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X, y = load_data()\n",
    "\n",
    "# perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-boating",
   "metadata": {},
   "source": [
    "### Step 2: Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "effective-anaheim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate transformers and classifier\n",
    "vect = CountVectorizer(tokenizer= tokenize)\n",
    "tfidf = TfidfTransformer()\n",
    "clf = RandomForestClassifier(random_state= 0)\n",
    "\n",
    "# Fit and/or transform each to the data\n",
    "X_train_counts = vect.fit_transform(X_train)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "clf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-healing",
   "metadata": {},
   "source": [
    "### Step 3: Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "elementary-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform test data\n",
    "X_test_counts = vect.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "\n",
    "# Predict test labels\n",
    "y_test_pred = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-phrase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bearing-there",
   "metadata": {},
   "source": [
    "### Step 4: Display results\n",
    "Display a confusion matrix and accuracy score based on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daily-alaska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 85   0  34]\n",
      " [  2  31   6]\n",
      " [  3   2 438]]\n",
      "Accuracy: 0.922\n"
     ]
    }
   ],
   "source": [
    "labels = np.unique(y_test_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_test_pred, labels= labels)\n",
    "accuracy = (y_test == y_test_pred).mean().round(3)\n",
    "\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-curtis",
   "metadata": {},
   "source": [
    "# Final Step: Refactor\n",
    "Organize these steps into the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "prerequisite-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(y_test, y_pred):\n",
    "    labels = np.unique(y_test_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_test_pred, labels= labels)\n",
    "    accuracy = (y_test == y_test_pred).mean().round(3)\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "automotive-poultry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_naive():\n",
    "    # load data\n",
    "    X, y = load_data()\n",
    "    # Split the data into train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 6)\n",
    "    \n",
    "    # Instantiate transformers and classifier\n",
    "    vect = CountVectorizer(tokenizer= tokenize)\n",
    "    tfidf = TfidfTransformer()\n",
    "    clf = RandomForestClassifier(random_state= 0)\n",
    "\n",
    "    # Fit and/or transform each to the data\n",
    "    X_train_counts = vect.fit_transform(X_train)\n",
    "    X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Transform test data\n",
    "    X_test_counts = vect.transform(X_test)\n",
    "    X_test_tfidf = tfidf.transform(X_test_counts)\n",
    "    # Predict test labels\n",
    "    y_test_pred = clf.predict(X_test_tfidf)\n",
    "    \n",
    "    display_results(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fabulous-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 18   5  84]\n",
      " [  5   0  23]\n",
      " [ 67  28 371]]\n",
      "Accuracy: 0.647\n"
     ]
    }
   ],
   "source": [
    "main_naive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-conditioning",
   "metadata": {},
   "source": [
    "### Advantages of Using Pipeline:   \n",
    "1. Simplicity and Convencience   \n",
    "2. Optimizing Entire Workflow, including data transformation and modeling steps. \n",
    "3. Preventing Data leakage   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-explanation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "compressed-theology",
   "metadata": {},
   "source": [
    "## 3. Implementing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "supreme-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipe():\n",
    "    # Load the data\n",
    "    X, y = load_data()\n",
    "    # Split data into train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state= 6)\n",
    "\n",
    "    # Build pipeline\n",
    "    pipe = Pipeline(steps = [\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', RandomForestClassifier(random_state= 0))\n",
    "    ])\n",
    "        \n",
    "    # Train classifier\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test data\n",
    "    y_test_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Display results\n",
    "    display_results(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "consecutive-representative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 18   5  84]\n",
      " [  5   0  23]\n",
      " [ 67  28 371]]\n",
      "Accuracy: 0.647\n"
     ]
    }
   ],
   "source": [
    "main_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-firewall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "parental-riverside",
   "metadata": {},
   "source": [
    "## 4. Pipelines And Feature Unions\n",
    "- A `pipeline` performs a list of steps in a _linear sequence_, while a `feature union` performs a list of steps in _parallel_ and then combines their results.\n",
    "- In more complex workflows, multiple feature unions are often used within pipelines, and multiple pipelines are used within feature unions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-mixer",
   "metadata": {},
   "source": [
    "### Creating Custom Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-leadership",
   "metadata": {},
   "source": [
    "Let's build a `case normalizer`, which simply converts all text to lowercase. Remember, all estimators have a fit method, and since this is a transformer, it also has a transform method.   \n",
    "\n",
    "- __fit()__: This takes in a 2D array X for the feature data and a 1d array y for the target labels. Inside the fit method, we simply return self. This allows us to chain methods together, since the result on calling fit on the transformer is still the transformer object. This method is required to be compatible with scikit-learn.   \n",
    "\n",
    "- __transform()__: The transform function is where we include the code that, well, transforms the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "based-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseNormalizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Lowercase the text.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y= None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return pd.Series(X).apply(lambda x: x.lower()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cheap-emphasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['implementing', 'a', 'custom', 'transformer', 'from',\n",
       "       'scikit-learn'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_normalizer = CaseNormalizer()\n",
    "\n",
    "X = np.array(['Implementing', 'a', 'Custom', 'Transformer', 'from', 'SCIKIT-LEARN'])\n",
    "\n",
    "case_normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-background",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "extreme-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Determine if a text starts with a verb.\"\"\"\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-feeling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "indian-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_parallel():\n",
    "    pipeline = Pipeline(steps = [\n",
    "        ('gen_features', FeatureUnion(transformer_list= [\n",
    "            ('text_pipeline', Pipeline(steps= [\n",
    "                ('vect', CountVectorizer(tokenizer= tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "            \n",
    "            ('has_lead_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "        \n",
    "        ('rfclf', RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "violent-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = pipeline_parallel()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    display_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "impressed-forward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 18   4  95]\n",
      " [  2   3  25]\n",
      " [ 70  26 358]]\n",
      "Accuracy: 0.631\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-sapphire",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "junior-cherry",
   "metadata": {},
   "source": [
    "## 5. GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "instant-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            \n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('starting_verb', StartingVerbExtractor())\n",
    "        ])),\n",
    "    \n",
    "        ('clf', RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    parameters = {\n",
    "        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'features__text_pipeline__vect__max_features': (None, 5000),\n",
    "        #'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "        'clf__n_estimators': [50, 200],\n",
    "        'clf__min_samples_split': [2, 3, 4]\n",
    "    }\n",
    "\n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(pipeline, parameters)\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "three-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(cv, y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nBest Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "treated-daniel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[107   0  25]\n",
      " [  1  30   1]\n",
      " [  1   1 435]]\n",
      "Accuracy: 0.9517470881863561\n",
      "\n",
      "Best Parameters: {'clf__min_samples_split': 2, 'clf__n_estimators': 200, 'features__text_pipeline__vect__max_df': 0.75, 'features__text_pipeline__vect__max_features': 5000, 'features__text_pipeline__vect__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    display_results(model, y_test, y_pred)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-accreditation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
